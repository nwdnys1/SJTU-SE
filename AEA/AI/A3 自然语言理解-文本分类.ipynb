{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;应用系统体系架构 - 人工智能模块 </h1>\n",
    "\n",
    "<h1 align=center> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A3：自然语言理解 - 文本分类</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 引言\n",
    "\n",
    "在这个Notebook中，我们将使用 Keras 构建 多层感知机(Multilayer Perceptron) 和 CNN 模型进行文档分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.404 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jieba\n",
    "jieba.load_userdict('nlp/lib/dict.txt.big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: Energy\n",
      "category: Electronics\n",
      "category: Communication\n",
      "category: Mine\n",
      "category: Transport\n",
      "category: Medical\n"
     ]
    }
   ],
   "source": [
    "# 数据集按不同类别分组\n",
    "root = 'nlp/dataset/'\n",
    "# groupA：数据集较小  groupB：数据集较大\n",
    "group = 'groupA'\n",
    "\n",
    "texts = []\n",
    "categories = []\n",
    "\n",
    "for (path, dirs, fnames) in os.walk(root + group):\n",
    "    for fname in fnames:\n",
    "        category = fname[4:-4]\n",
    "        print(\"category: \" + category)\n",
    "        with open(os.path.join(path, fname), 'r', encoding='gbk', errors='ignore') as f:\n",
    "            item = f.read().split('\\n')\n",
    "            for i in item:\n",
    "                seg = jieba.cut(i)\n",
    "                seg = ' '.join(seg)\n",
    "                texts.append(seg)\n",
    "                categories.append(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 导入构建神经网络模型的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active code page: 65001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# 加载 Tensorboard 扩展，用于模型可视化\n",
    "%load_ext tensorboard\n",
    "\n",
    "# 设置 Keras 的 Backend。每次进行新的设置时，需要restart环境\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# 导入 Keras 需要用到包\n",
    "import tensorflow.keras as keras\n",
    "from datetime import datetime\n",
    "import tensorboard\n",
    "\n",
    "# 清理之前的log文件，以确保 Tensorboard 显示正确\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 获取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split     # 训练、测试集分类\n",
    "from sklearn.preprocessing import LabelEncoder           # 标签方法\n",
    "from keras.preprocessing.text import Tokenizer           # 文本编码方法\n",
    "from keras_preprocessing.sequence import pad_sequences   # 补零方法\n",
    "from keras.utils.np_utils import to_categorical           # One-Hot 编码方法\n",
    "\n",
    "# 选择数据集较小的一组，并设置最大特征长度\n",
    "if group == 'groupA':\n",
    "    n_sent = 8000\n",
    "elif group == 'groupB':\n",
    "    n_sent = 50200\n",
    "else:\n",
    "    raise(Exception(\"You set a wrong group!\"))\n",
    "    \n",
    "# 总类别数\n",
    "n_label = len(set(categories))\n",
    "\n",
    "# One-Hot 方式对标签组重新打标签，形如：[[1.,0.,0.],[0.,1.,0.],[0.,0.,1.]]\n",
    "encoder = LabelEncoder()\n",
    "encoded_cate = encoder.fit_transform(categories)\n",
    "label_cate = to_categorical(encoded_cate)\n",
    "\n",
    "# 过滤标点符号以及特殊字符\n",
    "tokenizer = Tokenizer(filters='。，、；：’“ ”—【】（）·★', split=' ')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "# 对每个词进行word2id 编码\n",
    "vocabs = tokenizer.word_index\n",
    "# 文本编码后结果\n",
    "seqs = tokenizer.texts_to_sequences(texts)\n",
    "# One-Hot 方式对文本编码\n",
    "oh_seqs = tokenizer.sequences_to_matrix(seqs)\n",
    "# 对于没有达到最大特征长度的句子，左边补零\n",
    "pad_oh_seqs = pad_sequences(oh_seqs, maxlen=n_sent)\n",
    "\n",
    "# 对结果划分为训练集和测试集\n",
    "x_train, x_test, y_train, y_test = train_test_split(pad_oh_seqs, label_cate, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 构建MLP模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               4096512   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 4,099,590\n",
      "Trainable params: 4,099,590\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 1s 66ms/step - loss: 1.7782 - accuracy: 0.2037\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.3869 - accuracy: 0.8056\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0802 - accuracy: 0.8935\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.8512 - accuracy: 0.9306\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6860 - accuracy: 0.9444\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5502 - accuracy: 0.9491\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4594 - accuracy: 0.9491\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3765 - accuracy: 0.9491\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3177 - accuracy: 0.9491\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2631 - accuracy: 0.9491\n"
     ]
    }
   ],
   "source": [
    "from keras import Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPooling2D\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        Input(shape=(n_sent,)),\n",
    "        Dense(512, input_shape=(len(vocabs)+1,), activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(n_label, activation='softmax'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, callbacks=[tensorboard_callback])\n",
    "model.save(\"model.keras\")\n",
    "\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## . 评估 MLP 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.526979923248291\n",
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 构建 CNN 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 8000, 300)         3942900   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 8000, 256)         230656    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 2667, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 2667, 128)         98432     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 889, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 889, 64)           24640     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 56896)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 56896)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 56896)             227584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               14565632  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 19,091,386\n",
      "Trainable params: 18,977,594\n",
      "Non-trainable params: 113,792\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 5s 354ms/step - loss: 1.9329 - accuracy: 0.2685\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.4566 - accuracy: 0.8611\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 1s 155ms/step - loss: 0.3042 - accuracy: 0.9352\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 1s 156ms/step - loss: 0.2475 - accuracy: 0.9444\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 1s 154ms/step - loss: 0.2748 - accuracy: 0.9491\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 1s 155ms/step - loss: 0.2763 - accuracy: 0.9259\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 0.2205 - accuracy: 0.9444\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 1s 155ms/step - loss: 0.1372 - accuracy: 0.9583\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 1s 157ms/step - loss: 0.2307 - accuracy: 0.9491\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 0.1420 - accuracy: 0.9398\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Convolution1D, MaxPool1D,  BatchNormalization\n",
    "\n",
    "EMB_SIZE = 300\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        Input(shape=(n_sent,), dtype='float32'),\n",
    "        Embedding(len(vocabs) + 1, EMB_SIZE),\n",
    "        Convolution1D(filters=256, kernel_size=3, padding='same'),\n",
    "        MaxPool1D(3, 3, padding='same'),\n",
    "        Convolution1D(128, 3, padding='same'),\n",
    "        MaxPool1D(3, 3, padding='same'),\n",
    "        Convolution1D(64, 3, padding='same'),\n",
    "        Flatten(),\n",
    "        Dropout(0.1),\n",
    "        # 批量标准化，作用：通过对训练权重标准化，使得值域范围更接近正态分布，防止边缘极值情况导致的梯度消失等问题，从而使的模型更快收敛\n",
    "        BatchNormalization(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(n_label, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, callbacks=[tensorboard_callback])\n",
    "model.save(\"model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 评估  模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.212170124053955\n",
      "Test accuracy: 0.5833333134651184\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 构建 TextCNN 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 8000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 8000, 300)    3942900     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 8000, 2)      1802        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 2000, 2)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 2000, 2)      18          max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 500, 2)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 500, 2)       22          max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 125, 2)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2625, 2)      0           max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 5250)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 5250)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 6)            31506       dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,976,248\n",
      "Trainable params: 3,976,248\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 2s 183ms/step - loss: 1.7970 - accuracy: 0.2083\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 1.7753 - accuracy: 0.2500\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 1.7783 - accuracy: 0.2500\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 1.7560 - accuracy: 0.2500\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 1.7502 - accuracy: 0.2500\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 1.7535 - accuracy: 0.2083\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 1.7529 - accuracy: 0.2222\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 1.7528 - accuracy: 0.2407\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 1.7699 - accuracy: 0.2500\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 1.7555 - accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Concatenate\n",
    "from keras import Model\n",
    "\n",
    "# 输入层\n",
    "fin = Input(shape=(n_sent,), dtype='float32')\n",
    "\n",
    "# 词向量Embedding层\n",
    "embed = Embedding(len(vocabs) + 1, EMB_SIZE)(fin)\n",
    "\n",
    "# 1D卷积层-1\n",
    "cnn1 = Convolution1D(filters=2, kernel_size=3, padding='same', strides=1, activation='relu')(embed)\n",
    "# 1D Max Pooling层-1\n",
    "mp1 = MaxPool1D(pool_size=4)(cnn1)\n",
    "\n",
    "# 1D卷积层-2\n",
    "cnn2 = Convolution1D(2, 4, padding='same', strides=1, activation='relu')(mp1)\n",
    "# 1D Max Pooling层-2\n",
    "mp2 = MaxPool1D(pool_size=4)(cnn2)\n",
    "\n",
    "# 1D卷积层-3\n",
    "cnn3 = Convolution1D(2, 5, padding='same', strides=1, activation='relu')(mp2)\n",
    "# 1D Max Pooling层-3\n",
    "mp3 = MaxPool1D(pool_size=4)(cnn3)\n",
    "\n",
    "# 合并3个Max Pooling 层\n",
    "cnn = Concatenate(axis=1)([mp1, mp2, mp3])\n",
    "# 降维（展平）数据矩阵\n",
    "flat = Flatten()(cnn)\n",
    "# 丢弃20%神经元\n",
    "dropout = Dropout(0.2)(flat)\n",
    "\n",
    "# 全链接层\n",
    "fout = Dense(n_label, activation='softmax')(dropout)\n",
    "# 模型输出\n",
    "model = Model(inputs=fin, outputs=fout)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, callbacks=[tensorboard_callback])\n",
    "model.save(\"model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 评估 TextCNN 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.776894450187683\n",
      "Test accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
